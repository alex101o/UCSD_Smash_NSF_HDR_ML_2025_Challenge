{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4bbc134-c729-450d-b8f7-fb3121dc9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import cv2\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from ultralytics import SAM\n",
    "\n",
    "import io\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16845cc6-d820-4789-9f62-8512c740b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNCOMMENT AND RUN THIS IF YOU DO NOT HAVE ULTRALYTICS OR CV2 PIP INSTALLED!!!\n",
    "#pip install ultralytics, opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5187603b-69a9-4e41-b1ae-d9bf9802d3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"hackdata/sentinel-beetles/\")\n",
    "val_df = pd.read_csv(root / \"public_release\" / \"val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39e99bec-be1d-4597-be12-94c937f3439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## general arguments for the model\n",
    "def get_training_args(argv=None):\n",
    "    p = ArgumentParser()\n",
    "\n",
    "    # paths\n",
    "    p.add_argument(\"--train_csv\", type=str, default=\"hackdata/sentinel-beetles/public_release/train.csv\")\n",
    "    p.add_argument(\"--val_csv\", type=str, default=\"hackdata/sentinel-beetles/public_release/val.csv\")\n",
    "    p.add_argument(\"--train_img_dir\", type=str, default=\"training_images\")\n",
    "    p.add_argument(\"--val_img_dir\", type=str, default=\"validation_images\")\n",
    "    p.add_argument(\"--save_dir\", type=str, default=\"ckpts\")\n",
    "\n",
    "    # columns\n",
    "    p.add_argument(\"--event_col\", type=str, default=\"eventID\")\n",
    "    p.add_argument(\"--img_col\", type=str, default=\"relative_img_loc\")\n",
    "\n",
    "    # targets\n",
    "    p.add_argument(\"--spei30_col\", type=str, default=\"SPEI_30d\")\n",
    "    p.add_argument(\"--spei1y_col\", type=str, default=\"SPEI_1y\")\n",
    "    p.add_argument(\"--spei2y_col\", type=str, default=\"SPEI_2y\")\n",
    "\n",
    "    # model / data\n",
    "    p.add_argument(\"--img_size\", type=int, default=224)\n",
    "    p.add_argument(\"--k_max\", type=int, default=8)          # max images per event used\n",
    "    p.add_argument(\"--batch_size\", type=int, default=1)     # events per batch (keep small!)\n",
    "    p.add_argument(\"--num_workers\", type=int, default=1)\n",
    "    p.add_argument(\"--seed\", type=int, default=0)\n",
    "\n",
    "    # optimization\n",
    "    p.add_argument(\"--epochs\", type=int, default=30)\n",
    "    p.add_argument(\"--lr\", type=float, default=3e-5)        # good default for finetuning convnext_small\n",
    "    p.add_argument(\"--weight_decay\", type=float, default=0.05)\n",
    "    p.add_argument(\"--grad_accum\", type=int, default=1)     # increase if OOM\n",
    "    p.add_argument(\"--freeze_backbone_epochs\", type=int, default=1)  # stabilize early training\n",
    "\n",
    "    return p.parse_args(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4880fa2-361d-48b4-8782-3c8f41b78d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculating the r^2 score\n",
    "def r2_score_np(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    ss_res = float(((y_true - y_pred) ** 2).sum())\n",
    "    ss_tot = float(((y_true - y_true.mean()) ** 2).sum())\n",
    "    if ss_tot == 0.0:\n",
    "        return 0.0\n",
    "    return 1.0 - ss_res / ss_tot\n",
    "\n",
    "def evaluate_spei_r2(gts: np.ndarray, preds: np.ndarray) -> Tuple[float, float, float]:\n",
    "    return (\n",
    "        r2_score_np(gts[:, 0], preds[:, 0]),\n",
    "        r2_score_np(gts[:, 1], preds[:, 1]),\n",
    "        r2_score_np(gts[:, 2], preds[:, 2]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a760f32-e4a1-4df4-9c97-b02762ed9546",
   "metadata": {},
   "outputs": [],
   "source": [
    "## probably don't use this\n",
    "#class RandomRotate90:\n",
    "#    def __init__(self, angles=(0, 90, 180, 270), p=1.0):\n",
    "#        self.angles = angles\n",
    "#        self.p = p\n",
    "\n",
    "#    def __call__(self, img):\n",
    "#        if random.random() > self.p:\n",
    "#            return img\n",
    "#        angle = random.choice(self.angles)\n",
    "#        return TF.rotate(img, angle, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20ec88a8-a05f-49da-a967-1b694c0a2c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## grabs the .csv, where to look for the images, etc\n",
    "\n",
    "class EventDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each __getitem__ returns:\n",
    "      x: (K, 3, H, W) tensor of beetle images for this event (padded if <K)\n",
    "      mask: (K,) float mask where 1 = real image, 0 = padding\n",
    "      y: (3,) float targets for the event\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path: str, img_dir: str, event_col: str, img_col: str, target_cols: Tuple[str, str, str], tfm, k_max: int, train_mode: bool, seed: int = 0,):\n",
    "        self.df = pd.read_csv(csv_path).reset_index(drop=True)\n",
    "        self.img_root = Path(img_dir)\n",
    "        self.event_col = event_col\n",
    "        self.img_col = img_col\n",
    "        self.target_cols = target_cols\n",
    "        self.tfm = tfm\n",
    "        self.k_max = k_max\n",
    "        self.train_mode = train_mode\n",
    "        self.rng = random.Random(seed)\n",
    "\n",
    "        # sanity columns\n",
    "        for col in [event_col, img_col, *target_cols]:\n",
    "            if col not in self.df.columns:\n",
    "                raise KeyError(f\"Missing column '{col}' in {csv_path}. Columns: {list(self.df.columns)}\")\n",
    "\n",
    "        # drop missing\n",
    "        self.df = self.df.dropna(subset=[event_col, img_col, *target_cols]).reset_index(drop=True)\n",
    "\n",
    "        \n",
    "        # Build event -> row indices\n",
    "        self.event_to_rows: Dict[str, List[int]] = {}\n",
    "        for i in range(len(self.df)):\n",
    "            ev = str(self.df.loc[i, self.event_col])\n",
    "            self.event_to_rows.setdefault(ev, []).append(i)\n",
    "        self.events = list(self.event_to_rows.keys())\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.events)\n",
    "\n",
    "    def _open_image(self, rel: str) -> Image.Image:\n",
    "        rel = str(rel).lstrip(\"/\").replace(\"\\\\\", \"/\")\n",
    "        p = self.img_root / rel\n",
    "        if not p.exists():\n",
    "            # fallback: basename only\n",
    "            p2 = self.img_root / Path(rel).name\n",
    "            if p2.exists():\n",
    "                p = p2\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Image not found: {p} (also tried {p2})\")\n",
    "        return Image.open(p).convert(\"RGB\")\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        ev = self.events[idx]\n",
    "        rows = self.event_to_rows[ev]\n",
    "    \n",
    "        # targets from first row\n",
    "        row0 = self.df.loc[rows[0]]\n",
    "        y = torch.tensor([row0[c] for c in self.target_cols], dtype=torch.float32)\n",
    "    \n",
    "        # sample up to k_max\n",
    "        if self.train_mode and len(rows) > self.k_max:\n",
    "            chosen = self.rng.sample(rows, self.k_max)\n",
    "        else:\n",
    "            chosen = rows[:self.k_max]\n",
    "    \n",
    "        xs = []\n",
    "    \n",
    "        for r in chosen:\n",
    "            rel_path = self.df.loc[r, self.img_col]\n",
    "            img = self._open_image(rel_path)  # returns PIL or np array\n",
    "    \n",
    "            # MaskTransform expects np.ndarray (H x W x C)\n",
    "            if isinstance(img, Image.Image):\n",
    "                img = np.array(img)[:, :, ::-1]  # PIL RGB -> BGR for your MaskTransform\n",
    "    \n",
    "            x = self.tfm(img)  # returns Tensor [C, H, W]\n",
    "            xs.append(x)\n",
    "    \n",
    "        n = len(xs)\n",
    "        H = W = self.tfm.final_size if hasattr(self.tfm, \"final_size\") else xs[0].shape[1]\n",
    "    \n",
    "        # pad to k_max\n",
    "        if n < self.k_max:\n",
    "            pad = torch.zeros((self.k_max - n, 3, H, W), dtype=torch.float32)\n",
    "            x = torch.cat([torch.stack(xs, dim=0), pad], dim=0)\n",
    "        else:\n",
    "            x = torch.stack(xs[:self.k_max], dim=0)\n",
    "    \n",
    "        # simple mask: 1 for real images, 0 for padded\n",
    "        mask = torch.zeros((self.k_max,), dtype=torch.float32)\n",
    "        mask[:n] = 1.0\n",
    "\n",
    "        return x, mask, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "428d768e-4d8d-4869-bfe1-e4eee8c74e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defines the ConvNeXt Regressor to output 3 SPEI predictions (30d, 1y, 2y)\n",
    "class EventConvNeXtRegressor(nn.Module):\n",
    "    def __init__(self, backbone_name: str = \"convnext_small\"):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\"convnext_small\", pretrained=True, num_classes=0, cache_dir=str(Path(\"timm_cache\").resolve()),\n",
    ")\n",
    "        d = self.backbone.num_features\n",
    "        self.head = nn.Linear(d, 6)  # 3 mu + 3 log_sigma\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (B, K, 3, H, W)\n",
    "        mask: (B, K) 1 for real, 0 for padded\n",
    "        \"\"\"\n",
    "        B, K, C, H, W = x.shape\n",
    "        feats = self.backbone(x.view(B * K, C, H, W))  # (B*K, d)\n",
    "        d = feats.shape[-1]\n",
    "        feats = feats.view(B, K, d)             # (B, K, d)\n",
    "\n",
    "        # masked mean pool\n",
    "        m = mask.unsqueeze(-1)                  # (B, K, 1)\n",
    "        denom = m.sum(dim=1).clamp_min(1.0)     # (B, 1)\n",
    "        event_feat = (feats * m).sum(dim=1) / denom  # (B, d)\n",
    "\n",
    "        out = self.head(event_feat)             # (B, 6)\n",
    "        mu = out[:, :3]                         # (B, 3)\n",
    "        sigma = F.softplus(out[:, 3:]) + 1e-6\n",
    "\n",
    "        print(\"EventConvNeXtRegressor\")\n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7216a6a-2904-43be-a097-17c3d7f2a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining the Gaussian negative log-likelihood\n",
    "\n",
    "def gaussian_nll(y: torch.Tensor, mu: torch.Tensor, sigma: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    y, mu, sigma: (B, 3)\n",
    "    Negative log-likelihood for independent Gaussian dims.\n",
    "    \"\"\"\n",
    "    # 0.5*((y-mu)/sigma)^2 + log(sigma)\n",
    "    return (0.5 * ((y - mu) / sigma).pow(2) + torch.log(sigma)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "571ff4b4-cfc1-4552-b2ff-1b7bf8dda406",
   "metadata": {},
   "outputs": [],
   "source": [
    "## masking function, uses SAM to segment beetles, normalize size turned into a PyTorch transformation\n",
    "\n",
    "class MaskTransform:\n",
    "    \n",
    "    def __init__(self, model_path: str = \"sam2.1_l.pt\", final_size: int = 512, target_fraction: float = 0.67, device=\"cuda\"):\n",
    "        self.model = SAM(model_path)\n",
    "        self.final_size = final_size\n",
    "        self.target_fraction = target_fraction\n",
    "\n",
    "    def __call__(self, img: np.ndarray) -> torch.Tensor:        #img: H x W x C numpy array (BGR), returns: C x H x W torch tensor\n",
    "        \n",
    "        H, W = img.shape[:2]\n",
    "        attempt_counter = 0\n",
    "        best_mask = None\n",
    "\n",
    "        # try a few times to get a valid mask\n",
    "        while attempt_counter < 10 and best_mask is None:\n",
    "            points = [[W * (0.5+random.uniform(-0.1, 0.1)), H * (0.35+random.uniform(-0.1, 0.1))], [5, 5]]\n",
    "            labels = [1, 0]\n",
    "\n",
    "            with contextlib.redirect_stdout(io.StringIO()):\n",
    "                results = self.model(img, points=points, labels=labels)\n",
    "            r = results[0]\n",
    "\n",
    "            if r.masks is None:\n",
    "                attempt_counter += 1\n",
    "                continue\n",
    "\n",
    "            masks = r.masks.data.cpu().numpy()\n",
    "            best_area = 0\n",
    "            for m in masks:\n",
    "                area = m.sum()\n",
    "                area_ratio = area / (W * H)\n",
    "                if 0.25 < area_ratio < 0.75:\n",
    "                    if area > best_area:\n",
    "                        best_area = area\n",
    "                        best_mask = m\n",
    "\n",
    "            attempt_counter += 1\n",
    "\n",
    "        if best_mask is None:\n",
    "            # fallback: return resized original\n",
    "            img_resized = cv2.resize(img, (self.final_size, self.final_size))\n",
    "            img_resized = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
    "            return transforms.ToTensor()(img_resized)\n",
    "\n",
    "        # smoothing + polishing\n",
    "        mask = binary_fill_holes(best_mask).astype(np.uint8)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
    "        mask = cv2.dilate(mask, np.ones((3,3), np.uint8), iterations=1)\n",
    "\n",
    "        ys, xs = np.where(mask > 0)\n",
    "        if len(xs) == 0 or len(ys) == 0:\n",
    "            img_resized = cv2.resize(img, (self.final_size, self.final_size))\n",
    "            img_resized = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
    "            return transforms.ToTensor()(img_resized)\n",
    "\n",
    "        ymin, ymax = ys.min(), ys.max()\n",
    "        xmin, xmax = xs.min(), xs.max()\n",
    "\n",
    "        beetle = img[ymin:ymax+1, xmin:xmax+1]\n",
    "        beetle_mask = mask[ymin:ymax+1, xmin:xmax+1]\n",
    "\n",
    "        # scale normalization\n",
    "        h, w = beetle.shape[:2]\n",
    "        target_size = int(self.target_fraction * self.final_size)\n",
    "        scale = target_size / max(h, w)\n",
    "        new_h, new_w = int(h * scale), int(w * scale)\n",
    "\n",
    "        beetle = cv2.resize(beetle, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "        beetle_mask = cv2.resize(beetle_mask, (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # white square padding\n",
    "        side = max(new_h, new_w)\n",
    "        square = np.ones((side, side, 3), dtype=np.uint8) * 255\n",
    "        yoff = (side - new_h) // 2\n",
    "        xoff = (side - new_w) // 2\n",
    "        square[yoff:yoff+new_h, xoff:xoff+new_w][beetle_mask > 0] = beetle[beetle_mask > 0]\n",
    "\n",
    "        # final resize\n",
    "        square = cv2.resize(square, (self.final_size, self.final_size), interpolation=cv2.INTER_AREA)\n",
    "        square = cv2.cvtColor(square, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        return transforms.ToTensor()(square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcf0e0f0-341e-44ef-af3c-ae024b48726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_only(args):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    val_tfm = args.val_tfm\n",
    "    val_tfm._img_size = args.img_size\n",
    "\n",
    "    # Dataset\n",
    "    val_ds = EventDataset(\n",
    "        csv_path=args.val_csv,\n",
    "        img_dir=args.val_img_dir,\n",
    "        event_col=args.event_col,\n",
    "        img_col=args.img_col,\n",
    "        target_cols=(args.spei30_col, args.spei1y_col, args.spei2y_col),\n",
    "        tfm=args.val_tfm,\n",
    "        k_max=args.k_max,\n",
    "        train_mode=False,\n",
    "        seed=args.seed,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=(device.type == \"cuda\")\n",
    "    )\n",
    "\n",
    "    # Load model (make sure to get rid of pretrained = True)\n",
    "    model = EventConvNeXtRegressor(\"convnext_small\").to(device)\n",
    "    model.load_state_dict(torch.load(args.model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    va_loss = 0.0\n",
    "    va_preds, va_gts = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
    "        for step, (x, mask, y) in enumerate(pbar):\n",
    "            x = x.to(device, non_blocking=True)           # [B, K, 3, H, W]\n",
    "            mask = mask.to(device, non_blocking=True)    # [B, K]\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            # Model forward\n",
    "            mu, sigma = model(x, mask)  # Adjust your model forward to handle mask shape [B, K]\n",
    "\n",
    "            # Compute Gaussian NLL loss\n",
    "            loss = gaussian_nll(y, mu, sigma)\n",
    "            va_loss += float(loss.item())\n",
    "\n",
    "            # Collect predictions for R² evaluation\n",
    "            va_preds.append(mu.detach().cpu().numpy())\n",
    "            va_gts.append(y.detach().cpu().numpy())\n",
    "\n",
    "            pbar.set_postfix({\"loss\": va_loss / (step + 1)})\n",
    "\n",
    "    # Concatenate all predictions/ground truths\n",
    "    va_preds = np.concatenate(va_preds, axis=0)\n",
    "    va_gts = np.concatenate(va_gts, axis=0)\n",
    "\n",
    "    # Compute R² metrics\n",
    "    v30, v1y, v2y = evaluate_spei_r2(va_gts, va_preds)\n",
    "    avg = (v30 + v1y + v2y) / 3.0\n",
    "\n",
    "    print(f\"Validation | val_loss={va_loss/len(val_loader):.4f} \"\n",
    "          f\"val_r2=({v30:.3f},{v1y:.3f},{v2y:.3f}) avg={avg:.3f}\")\n",
    "\n",
    "    return va_preds, va_gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "207690f8-3ee0-4e93-8f08-65d07f53cfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5536/3039291672.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(args.model_path, map_location=device))\n",
      "Validation:   0%|          | 0/739 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "0: 1024x1024 1 0, 49611.8ms\n",
      "Speed: 49.6ms preprocess, 49611.8ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 50376.5ms\n",
      "Speed: 59.6ms preprocess, 50376.5ms inference, 6.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 50737.6ms\n",
      "Speed: 69.3ms preprocess, 50737.6ms inference, 14.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 1 1, 51143.0ms\n",
      "Speed: 75.4ms preprocess, 51143.0ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 51462.9ms\n",
      "Speed: 9.9ms preprocess, 51462.9ms inference, 6.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 50293.3ms\n",
      "Speed: 10.3ms preprocess, 50293.3ms inference, 2.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 1 1, 52010.6ms\n",
      "Speed: 15.1ms preprocess, 52010.6ms inference, 5.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 52462.4ms\n",
      "Speed: 19.7ms preprocess, 52462.4ms inference, 13.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 51191.9ms\n",
      "Speed: 12.9ms preprocess, 51191.9ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 51979.3ms\n",
      "Speed: 14.3ms preprocess, 51979.3ms inference, 6.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 51357.9ms\n",
      "Speed: 13.9ms preprocess, 51357.9ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 52941.6ms\n",
      "Speed: 19.2ms preprocess, 52941.6ms inference, 16.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 50365.0ms\n",
      "Speed: 16.0ms preprocess, 50365.0ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 51488.5ms\n",
      "Speed: 13.7ms preprocess, 51488.5ms inference, 4.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 51808.2ms\n",
      "Speed: 13.0ms preprocess, 51808.2ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 52537.8ms\n",
      "Speed: 20.2ms preprocess, 52537.8ms inference, 17.2ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 51304.7ms\n",
      "Speed: 14.0ms preprocess, 51304.7ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 51732.1ms\n",
      "Speed: 12.7ms preprocess, 51732.1ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "0: 1024x1024 1 0, 51255.8ms\n",
      "Speed: 12.2ms preprocess, 51255.8ms inference, 1.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "\n",
      "0: 1024x1024 1 0, 52532.5ms\n",
      "Speed: 20.4ms preprocess, 52532.5ms inference, 14.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "img_size = 512\n",
    "\n",
    "class Args:\n",
    "    ## dataset\n",
    "    val_csv = \"hackdata/sentinel-beetles/public_release/val.csv\"\n",
    "    val_img_dir = \"validation_images\"\n",
    "\n",
    "    ## .csv columns\n",
    "    img_col = \"relative_img_loc\"\n",
    "    event_col = \"eventID\"\n",
    "    spei30_col = \"SPEI_30d\"\n",
    "    spei1y_col = \"SPEI_1y\"\n",
    "    spei2y_col = \"SPEI_2y\"\n",
    "\n",
    "    ## transform\n",
    "    val_tfm = MaskTransform(model_path=\"sam2.1_l.pt\", final_size=img_size)\n",
    "\n",
    "    ## dataloader settings\n",
    "    batch_size = 1\n",
    "    img_size = img_size\n",
    "    k_max = 3\n",
    "    seed = 42\n",
    "    num_workers = 4\n",
    "\n",
    "    ## which model to use\n",
    "    model_path = \"event_convnext_small_img_aug_best.pth\"\n",
    "\n",
    "args = Args()\n",
    "preds, gts = validate_only(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72522f9-5a6b-4e4b-9bb9-e9c5fbaf4fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2f749-2b07-46d2-b151-508b27c50094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
